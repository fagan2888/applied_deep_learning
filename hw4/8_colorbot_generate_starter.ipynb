{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_colorbot_generate_starter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "CmO1WEDPp1xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3sd_AEJspQJg",
        "colab_type": "code",
        "outputId": "4f4e9640-518f-455f-afe4-10f4ead6b64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the colors dataset\n",
        "if not os.path.exists('colors.csv'):\n",
        "  !curl -O 'https://raw.githubusercontent.com/random-forests/datasets/master/colors.csv'\n",
        "!head colors.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name,red,green,blue\n",
            "parakeet,174,182,87\n",
            "saddle brown,88,52,1\n",
            "cucumber crush,222,237,215\n",
            "pool blue,134,194,201\n",
            "distance,98,110,130\n",
            "light urple,179,111,246\n",
            "east side,172,145,206\n",
            "florida seashells,250,228,199\n",
            "paris,145,167,189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mG0WnNZtmxul",
        "colab_type": "code",
        "outputId": "890be797-1779-47de-8a5c-795b0a50fc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "colors_rgb = []\n",
        "csv_reader = csv.reader(open('colors.csv'), delimiter=',')\n",
        "next(csv_reader) # Remove the header\n",
        "for row in csv_reader:\n",
        "    name, r, g, b = row[0].lower().strip(), int(row[1]), int(row[2]), int(row[3])\n",
        "    colors_rgb.append((name, r, g, b))\n",
        "print(len(colors_rgb), 'colors downloaded')\n",
        "print('For example', colors_rgb[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14157 colors downloaded\n",
            "For example ('parakeet', 174, 182, 87)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PdIWQdlBrZXA",
        "colab_type": "code",
        "outputId": "c2403fdf-6386-44a7-d359-b5c9b771e09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# In this experiment, we will train a char-baed RNN to generate a line of text\n",
        "# that resembles this dataset (we'll treat each line as a string)\n",
        "sentences = []\n",
        "for row in colors_rgb:\n",
        "  line = ' '.join([str(part) for part in row])\n",
        "  sentences.append(line)\n",
        "print(sentences[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parakeet 174 182 87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q_o2oIUNuyWZ",
        "colab_type": "code",
        "outputId": "c32330a8-ebec-43ec-f56b-2d8a672f4a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# vocabulary for our char-based RNN\n",
        "chars = set()\n",
        "for sentence in sentences:\n",
        "  for char in sentence:\n",
        "    chars.add(char)\n",
        "    \n",
        "# add a special char for padding\n",
        "chars.add('<pad>')\n",
        "\n",
        "vocab = sorted(set(chars))\n",
        "\n",
        "# Create a mapping from unique characters to indices\n",
        "char2idx = {u : i for i, u in enumerate(vocab)}\n",
        "idx2char = {i : u for i, u in enumerate(vocab)}\n",
        "\n",
        "# Vocab size\n",
        "vocab_size = len(vocab)\n",
        "print('vocab size:', vocab_size)\n",
        "print(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 38\n",
            "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p5sdKRJ6rp8I",
        "colab_type": "code",
        "outputId": "6e61bce6-7584-4a65-c30f-f757cbd5b880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# vectorize the text\n",
        "text_int = []\n",
        "for sentence in sentences:\n",
        "  int_sentence = [] \n",
        "  for c in sentence:\n",
        "    int_sentence.append(char2idx[c])\n",
        "  text_int.append(int_sentence)\n",
        "print('Vectorized sentence', text_int[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorized sentence [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0g6Do15As4yQ",
        "colab_type": "code",
        "outputId": "270e6be0-07c4-4381-bc17-e5b782f2b45c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# pad sentences to max_length\n",
        "max_length = 40\n",
        "for sentence in text_int:\n",
        "  while (len(sentence) < max_length):\n",
        "    sentence.append(char2idx['<pad>'])\n",
        "print('Padded sentences', text_int[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded sentences [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-y87WF_Fw1kJ",
        "colab_type": "code",
        "outputId": "f3886555-8175-43d5-c28d-77c680ccf8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# truncate all sentences to max_length\n",
        "for i in range(len(text_int)):\n",
        "  sentence = text_int[i]\n",
        "  if len(sentence) > max_length:\n",
        "    text_int[i] = sentence[:max_length]\n",
        "print(\"Truncated sentences\", text_int[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncated sentences [27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwu3FgSpxNWT",
        "colab_type": "code",
        "outputId": "aefdfe0e-74ea-4bd6-9d55-8fdc88ef25e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# Create training examples / targets\n",
        "input_text = []\n",
        "target_text = []\n",
        "\n",
        "for i in range(len(text_int)):\n",
        "  inps = text_int[i][:max_length-1]\n",
        "  targ = text_int[i][1:max_length]\n",
        "  input_text.append(inps)\n",
        "  target_text.append(targ)\n",
        "  \n",
        "print(\"First training example, target\")  \n",
        "print(input_text[0])\n",
        "print(target_text[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First training example, target\n",
            "[27, 12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
            "[12, 29, 12, 22, 16, 16, 31, 0, 2, 8, 5, 0, 2, 9, 3, 0, 9, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cjyeDrvGzl8E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TT8ed7cu0w-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Model, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x):\n",
        "    embedding = self.embedding(x)\n",
        "    prediction = self.fc(embedding)\n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jA8Pssrh1NKE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "# Here, this is basically just a trick to avoid having \n",
        "# to one-hot encode the characters\n",
        "# I don't think it will add much otherwise\n",
        "# this would be more useful if we had a much larger vocabulary\n",
        "embedding_dim = 128\n",
        "\n",
        "# Number of RNN units\n",
        "units = 256\n",
        "\n",
        "model = Model(vocab_size, embedding_dim, units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrSRbSQk1Rcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(labels, logits):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-AILeTt71UXr",
        "colab_type": "code",
        "outputId": "3a297655-d140-47b7-d3e4-7723d05dde0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "model.build(tf.TensorShape([BATCH_SIZE, max_length]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  4864      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  4902      \n",
            "=================================================================\n",
            "Total params: 9,766\n",
            "Trainable params: 9,766\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gjYtdulr8ukK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "# Checkpoint instance\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkLMi8GI1c9b",
        "colab_type": "code",
        "outputId": "25fdf454-0acc-4a7c-c80f-edc14ac6a04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    # initally hidden is None\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (input_seq, target_seq)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              predictions = model(input_seq)\n",
        "              loss = loss_function(target_seq, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time for epoch {} sec\\n'.format(time.time() - start))\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.6176\n",
            "Epoch 1 Batch 100 Loss 1.8123\n",
            "Epoch 1 Batch 200 Loss 1.4365\n",
            "Epoch 1 Loss 1.4116\n",
            "Time for epoch 3.9025301933288574 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4040\n",
            "Epoch 2 Batch 100 Loss 1.3777\n",
            "Epoch 2 Batch 200 Loss 1.3020\n",
            "Epoch 2 Loss 1.3510\n",
            "Time for epoch 3.6673996448516846 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3247\n",
            "Epoch 3 Batch 100 Loss 1.3668\n",
            "Epoch 3 Batch 200 Loss 1.3587\n",
            "Epoch 3 Loss 1.3240\n",
            "Time for epoch 3.483438730239868 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3187\n",
            "Epoch 4 Batch 100 Loss 1.3733\n",
            "Epoch 4 Batch 200 Loss 1.3483\n",
            "Epoch 4 Loss 1.3440\n",
            "Time for epoch 3.358915328979492 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3184\n",
            "Epoch 5 Batch 100 Loss 1.3546\n",
            "Epoch 5 Batch 200 Loss 1.3131\n",
            "Epoch 5 Loss 1.3634\n",
            "Time for epoch 3.3784596920013428 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3308\n",
            "Epoch 6 Batch 100 Loss 1.2753\n",
            "Epoch 6 Batch 200 Loss 1.2996\n",
            "Epoch 6 Loss 1.3508\n",
            "Time for epoch 3.3605141639709473 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2927\n",
            "Epoch 7 Batch 100 Loss 1.3282\n",
            "Epoch 7 Batch 200 Loss 1.2978\n",
            "Epoch 7 Loss 1.3673\n",
            "Time for epoch 3.872415542602539 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3165\n",
            "Epoch 8 Batch 100 Loss 1.3082\n",
            "Epoch 8 Batch 200 Loss 1.3585\n",
            "Epoch 8 Loss 1.3115\n",
            "Time for epoch 4.281810283660889 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.3248\n",
            "Epoch 9 Batch 100 Loss 1.3382\n",
            "Epoch 9 Batch 200 Loss 1.3129\n",
            "Epoch 9 Loss 1.3086\n",
            "Time for epoch 4.418702602386475 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.3468\n",
            "Epoch 10 Batch 100 Loss 1.2914\n",
            "Epoch 10 Batch 200 Loss 1.3386\n",
            "Epoch 10 Loss 1.3337\n",
            "Time for epoch 4.685222625732422 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0U_cmqGP8dwg",
        "colab_type": "code",
        "outputId": "ca853395-b2e9-47a8-c69c-510e5b1ad4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!ls {checkpoint_dir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t     ckpt-5.data-00000-of-00001\n",
            "ckpt-10.data-00000-of-00001  ckpt-5.index\n",
            "ckpt-10.index\t\t     ckpt-6.data-00000-of-00001\n",
            "ckpt-1.data-00000-of-00001   ckpt-6.index\n",
            "ckpt-1.index\t\t     ckpt-7.data-00000-of-00001\n",
            "ckpt-2.data-00000-of-00001   ckpt-7.index\n",
            "ckpt-2.index\t\t     ckpt-8.data-00000-of-00001\n",
            "ckpt-3.data-00000-of-00001   ckpt-8.index\n",
            "ckpt-3.index\t\t     ckpt-9.data-00000-of-00001\n",
            "ckpt-4.data-00000-of-00001   ckpt-9.index\n",
            "ckpt-4.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0K3VLTVT81rt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is a hack to let us use the model with a different \n",
        "# batch size later\n",
        "model = Model(vocab_size, embedding_dim, units)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Le_MRIFI1tRL",
        "colab_type": "code",
        "outputId": "177ba82c-6f40-4f96-a592-25d89e856810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step (generating text using the learned model)\n",
        "\n",
        "# Number of characters to generate\n",
        "num_generate = max_length\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = random.choice(string.ascii_lowercase)\n",
        "\n",
        "# Converting our start string to numbers (vectorizing) \n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# Empty string to store our results\n",
        "text_generated = []\n",
        "\n",
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "temperature = 0.5\n",
        "\n",
        "# Here batch size == 1\n",
        "model.reset_states()\n",
        "for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a multinomial distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "generated_color = start_string + ''.join(text_generated).replace('<pad>', '')\n",
        "print(generated_color)\n",
        "\n",
        "try:\n",
        "  parts = generated_color.split()\n",
        "  r = float(parts[-3])\n",
        "  g = float(parts[-2])\n",
        "  b = float(parts[-1])\n",
        "  plt.clf()\n",
        "  _ = plt.imshow([[(r, g, b)]])\n",
        "  _ = plt.axis('off')\n",
        "  _ = plt.title(generated_color, fontsize=18)\n",
        "except:\n",
        "  print('unable to parse color')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ce 18 16 1 15 24 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAFdCAYAAABcnZV9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKxVIlx3hlwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f89d5f7d2e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "2LSxcAKVBNLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}